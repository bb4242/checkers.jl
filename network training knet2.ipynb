{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Method definition ==(Base.Nullable{S}, Base.Nullable{T}) in module Base at nullable.jl:238 overwritten in module NullableArrays at /home/bbethke/.julia/v0.6/NullableArrays/src/operators.jl:99.\n"
     ]
    }
   ],
   "source": [
    "import SQLite\n",
    "using Knet\n",
    "import Blosc\n",
    "import MsgPack"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SQLite.DB(\"games.sqlite\")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db = SQLite.DB(\"games.sqlite\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "positions = SQLite.query(db, \"select positions.*, games.outcome from positions, games where positions.game_id=games.id order by id limit 10000\");\n",
    "positions[:, :state_tensor] = @. Array{Float32}(reshape(Blosc.decompress(UInt8, get(positions[:, :board_state])), 8, 4, 8));\n",
    "positions[:, :moves_tensor] = @. Array{Float32}(reshape(Blosc.decompress(Float16, get(positions[:, :mcts_moves])), 8, 4, 4));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = size(positions, 1)\n",
    "x = cat(4, positions[1:n, :state_tensor]...);\n",
    "y_moves = cat(4, positions[1:n, :moves_tensor]...);\n",
    "y_outcome = cat(4, positions[1:n, :outcome]...);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ConvLayer (generic function with 1 method)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function ConvLayer(n_filters, kernel_shape, input_shape; activation=identity, T=Float32)\n",
    "    θ = [\n",
    "        randn(T, kernel_shape[1], kernel_shape[2], input_shape[3], n_filters),\n",
    "        randn(T, 1, 1, n_filters, 1)\n",
    "    ]\n",
    "    ϕ = (θ_, x) -> activation.( conv4(θ_[1], x, padding=div(kernel_shape[1], 2)) .+ θ_[2] )\n",
    "    output_shape = size(ϕ(θ, zeros(T, input_shape..., 1)), 1, 2, 3)\n",
    "    return Dict(\n",
    "        :θ => θ,\n",
    "        :ϕ => ϕ,\n",
    "        :input_shape => input_shape,\n",
    "        :output_shape => output_shape\n",
    "    )\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "softmax (generic function with 1 method)"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function softmax(x, axes)\n",
    "    exp_x = exp.(x .- maximum(x, axes))\n",
    "    return exp_x ./ sum(exp_x, axes)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SoftmaxLayer (generic function with 1 method)"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function SoftmaxLayer(axes, input_shape)\n",
    "    θ = []\n",
    "    ϕ = (θ_, x) -> softmax(x, axes)\n",
    "    return Dict(\n",
    "        :θ => θ,\n",
    "        :ϕ => ϕ,\n",
    "        :input_shape => input_shape,\n",
    "        :output_shape => input_shape\n",
    "    )    \n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "apply (generic function with 1 method)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function apply(θs, ϕs, x)\n",
    "    for i in 1:length(θs)\n",
    "        x = ϕs[i](θs[i], x)\n",
    "    end\n",
    "    return x\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "model (generic function with 1 method)"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(layers) = [l[:θ] for l in layers], [l[:ϕ] for l in layers]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "cl1 = ConvLayer(16, (3, 3), (8, 4, 8), activation=relu);\n",
    "cl2 = ConvLayer(4, (3, 3), (8, 4, 16), activation=relu);\n",
    "sm1 = SoftmaxLayer([1, 2, 3], (8, 4, 4));\n",
    "θs, ϕs = model([cl1, cl2, sm1]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "loss (generic function with 1 method)"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss(θs, ϕs, x, y) = mean(abs2, apply(θs, ϕs, x) .- y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.008126385f0"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss(θs, ϕs, x, y_moves)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(::gradfun) (generic function with 1 method)"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "∇loss = grad(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3-element Array{Any,1}:\n",
       " Any[Float32[-0.000127309 0.00014844 0.000198423; 7.84476f-5 0.000181421 2.60013f-5; -1.55478f-6 -1.55466f-6 -8.49236f-7]\n",
       "\n",
       "Float32[0.0 0.0 0.0; -1.73102f-12 -1.73102f-12 -1.73102f-12; -0.000246022 -0.000211334 -0.000211334]\n",
       "\n",
       "Float32[0.0 0.0 0.0; 0.0 0.0 0.0; 0.0 0.0 0.0]\n",
       "\n",
       "Float32[0.0 0.0 0.0; 0.0 0.0 0.0; 0.0 0.0 0.0]\n",
       "\n",
       "Float32[-0.000246022 -0.000289058 -0.000340393; -0.000451545 -0.000321805 -0.000167851; -0.00012552 7.25054f-5 7.03342f-5]\n",
       "\n",
       "Float32[1.23196f-6 1.23196f-6 -1.20404f-7; 1.46577f-6 1.46577f-6 -7.46328f-14; 1.46577f-6 1.46577f-6 -7.46328f-14]\n",
       "\n",
       "Float32[0.0 0.0 0.0; 0.0 0.0 0.0; 0.0 0.0 0.0]\n",
       "\n",
       "Float32[-0.000374563 -0.000141849 -0.000141849; -0.000374563 -0.000141849 -0.000141849; -0.000374563 -0.000141849 -0.000141849]\n",
       "\n",
       "Float32[0.000133302 8.31798f-5 -0.000315019; 6.74224f-5 0.000134769 9.48618f-7; -1.11804f-6 -1.11796f-6 -1.21584f-6]\n",
       "\n",
       "Float32[9.26293f-12 8.27723f-12 8.27723f-12; 0.000120424 0.000216 0.000216; 2.20555f-5 6.47465f-5 6.50661f-5]\n",
       "\n",
       "Float32[0.0 0.0 0.0; 0.0 0.0 0.0; 0.0 0.0 0.0]\n",
       "\n",
       "Float32[0.0 0.0 0.0; 0.0 0.0 0.0; 0.0 0.0 0.0]\n",
       "\n",
       "Float32[2.20555f-5 -2.84803f-6 0.000395061; -3.22252f-5 -0.000270174 -0.000136741; 0.000134683 1.69667f-5 1.63565f-5]\n",
       "\n",
       "Float32[1.25038f-7 1.25037f-7 -1.64851f-7; 3.88513f-7 3.88513f-7 9.16583f-13; 3.88513f-7 3.88513f-7 9.16583f-13]\n",
       "\n",
       "Float32[0.0 0.0 0.0; 0.0 0.0 0.0; 0.0 0.0 0.0]\n",
       "\n",
       "Float32[0.000155232 8.02067f-5 8.02067f-5; 0.000155232 8.02067f-5 8.02067f-5; 0.000155232 8.02067f-5 8.02067f-5]\n",
       "\n",
       "Float32[8.90483f-5 5.56207f-5 3.41645f-5; -1.17551f-5 -0.000156645 -0.000133137; -1.34819f-11 -1.78677f-10 -2.94655f-10]\n",
       "\n",
       "Float32[-3.23735f-12 -2.91093f-12 -3.73046f-12; 6.50065f-5 4.41308f-5 4.41308f-5; 0.00018299 -0.000299148 -0.000299148]\n",
       "\n",
       "Float32[0.0 0.0 0.0; 0.0 0.0 0.0; 0.0 0.0 0.0]\n",
       "\n",
       "Float32[0.0 0.0 0.0; 0.0 0.0 0.0; 0.0 0.0 0.0]\n",
       "\n",
       "Float32[0.00018299 -0.000287395 -0.000265939; 0.000218787 -0.000119261 -0.000142768; 8.90483f-5 6.73737f-5 6.73739f-5]\n",
       "\n",
       "Float32[0.0 -1.66638f-13 -1.66638f-13; 0.0 -1.66638f-13 -1.66638f-13; 0.0 -1.66638f-13 -1.66638f-13]\n",
       "\n",
       "Float32[0.0 0.0 0.0; 0.0 0.0 0.0; 0.0 0.0 0.0]\n",
       "\n",
       "Float32[0.000272038 -0.000231775 -0.000231775; 0.000272038 -0.000231775 -0.000231775; 0.000272038 -0.000231775 -0.000231775]\n",
       "\n",
       "...\n",
       "\n",
       "Float32[5.46522f-5 0.00011234 0.000164586; -9.87405f-7 6.35873f-5 0.000117593; 2.35802f-8 2.36541f-8 -3.97392f-7]\n",
       "\n",
       "Float32[4.78796f-19 6.96186f-13 6.96186f-13; 4.78796f-19 -3.7855f-5 -3.7855f-5; 4.78796f-19 -3.7855f-5 -3.7855f-5]\n",
       "\n",
       "Float32[0.0 0.0 0.0; 0.0 0.0 0.0; 0.0 0.0 0.0]\n",
       "\n",
       "Float32[0.0 0.0 0.0; 0.0 0.0 0.0; 0.0 0.0 0.0]\n",
       "\n",
       "Float32[0.0 1.61203f-5 -3.7855f-5; 5.56565f-5 0.000102745 4.70886f-5; 5.46455f-5 0.000166308 0.000165079]\n",
       "\n",
       "Float32[6.77585f-7 6.77586f-7 -1.05107f-6; 6.94481f-7 6.94482f-7 -9.55425f-7; 6.94481f-7 6.94482f-7 -9.55425f-7]\n",
       "\n",
       "Float32[0.0 0.0 0.0; 0.0 0.0 0.0; 0.0 0.0 0.0]\n",
       "\n",
       "Float32[5.39746f-5 0.000127782 0.000127782; 5.39746f-5 0.000127782 0.000127782; 5.39746f-5 0.000127782 0.000127782]\n",
       "\n",
       "Float32[-9.28004f-7 4.51375f-5 4.49127f-5; 2.30133f-6 4.83669f-5 4.84165f-5; -4.06771f-6 -4.06766f-6 -2.34487f-6]\n",
       "\n",
       "Float32[9.50636f-18 -4.4586f-13 -4.4586f-13; 1.00878f-12 5.63457f-13 2.29534f-12; 3.01148f-7 3.01148f-7 8.17978f-7]\n",
       "\n",
       "Float32[0.0 0.0 0.0; 0.0 0.0 0.0; 0.0 0.0 0.0]\n",
       "\n",
       "Float32[0.0 0.0 0.0; 0.0 0.0 0.0; 0.0 0.0 0.0]\n",
       "\n",
       "Float32[3.01148f-7 3.01148f-7 8.17979f-7; -2.30134f-6 -2.30134f-6 -2.35098f-6; 3.76656f-6 4.98321f-5 4.75924f-5]\n",
       "\n",
       "Float32[-6.26856f-7 -6.26855f-7 -3.34895f-7; -1.90425f-12 -1.15019f-12 -4.68067f-13; -1.90425f-12 -1.15019f-12 -4.68067f-13]\n",
       "\n",
       "Float32[0.0 0.0 0.0; 0.0 0.0 0.0; 0.0 0.0 0.0]\n",
       "\n",
       "Float32[0.0 4.60656f-5 4.60656f-5; 0.0 4.60656f-5 4.60656f-5; 0.0 4.60656f-5 4.60656f-5]\n",
       "\n",
       "Float32[0.0 5.00235f-12 -4.1308f-5; 1.19825f-7 1.19825f-7 -6.7232f-8; 1.19825f-7 1.19825f-7 -6.7232f-8]\n",
       "\n",
       "Float32[7.7784f-12 8.10181f-12 6.141f-12; 7.7784f-12 -8.06887f-5 -8.06887f-5; -0.000111617 -0.000353236 -0.000350968]\n",
       "\n",
       "Float32[0.0 0.0 0.0; 0.0 0.0 0.0; 0.0 0.0 0.0]\n",
       "\n",
       "Float32[0.0 0.0 0.0; 0.0 0.0 0.0; 0.0 0.0 0.0]\n",
       "\n",
       "Float32[-0.000111617 -0.000353236 -0.00030966; -0.000111617 -0.000272547 -0.000270279; 0.0 0.0 0.0]\n",
       "\n",
       "Float32[-2.77764f-6 -2.77764f-6 -5.0961f-7; -2.65782f-6 -2.65782f-6 -5.76842f-7; -2.65782f-6 -2.65782f-6 -5.76842f-7]\n",
       "\n",
       "Float32[0.0 0.0 0.0; 0.0 0.0 0.0; 0.0 0.0 0.0]\n",
       "\n",
       "Float32[-0.000108839 -0.000350458 -0.000350458; -0.000108839 -0.000350458 -0.000350458; -0.000108839 -0.000350458 -0.000350458], Float32[-0.000140384]\n",
       "\n",
       "Float32[8.05952f-5]\n",
       "\n",
       "Float32[-0.000231775]\n",
       "\n",
       "...\n",
       "\n",
       "Float32[0.000128477]\n",
       "\n",
       "Float32[4.60656f-5]\n",
       "\n",
       "Float32[-0.000353116]]\n",
       " Any[Float32[-3.58889f-20 -4.34474f-13 -4.288f-13; -6.35549f-22 -2.88914f-15 -1.7289f-13; -2.44684f-24 -2.015f-13 -1.37909f-13]\n",
       "\n",
       "Float32[-7.3532f-20 -1.04301f-12 -1.2432f-12; -8.07355f-20 -1.07045f-12 -1.18356f-12; -7.99652f-20 -1.40613f-12 -1.3835f-12]\n",
       "\n",
       "Float32[-3.90745f-20 -3.94687f-13 -7.12814f-13; -5.03119f-20 -4.18808f-13 -7.49675f-13; -3.39874f-20 -1.59815f-13 -2.88807f-13]\n",
       "\n",
       "...\n",
       "\n",
       "Float32[-9.58991f-24 -3.84654f-13 -3.08229f-13; -2.331f-21 -3.027f-13 -1.63823f-24; -9.49455f-21 -1.89895f-13 -2.40199f-18]\n",
       "\n",
       "Float32[-1.27096f-25 -4.05024f-13 -7.64013f-13; -1.22551f-24 -5.98715f-14 -2.89945f-13; -1.5519f-20 -2.30969f-13 -7.1297f-14]\n",
       "\n",
       "Float32[-3.89638f-20 -5.79634f-13 -3.69178f-13; -2.92977f-20 -2.67119f-13 -2.45988f-15; -2.02722f-20 -4.18707f-13 -2.66266f-13]\n",
       "\n",
       "Float32[-6.31658f-25 -1.7685f-24 -1.89569f-24; -1.55094f-25 -1.57219f-24 -2.08543f-24; -1.6204f-25 -7.06041f-25 -5.5786f-25]\n",
       "\n",
       "Float32[-1.93997f-24 -5.59611f-25 -4.01075f-28; -1.70564f-24 -2.21467f-24 -1.33058f-24; -2.20993f-24 -2.81606f-24 -1.57145f-24]\n",
       "\n",
       "Float32[-3.59722f-24 -2.94595f-24 -6.52742f-25; -4.87382f-24 -2.46898f-24 -7.45264f-25; -3.4272f-24 -2.61898f-24 -3.15408f-24]\n",
       "\n",
       "...\n",
       "\n",
       "Float32[-1.70007f-24 -7.86732f-25 -2.05682f-24; -1.52211f-24 -4.35527f-28 -2.5344f-24; -8.39176f-25 -6.82635f-29 -1.27128f-24]\n",
       "\n",
       "Float32[-1.18012f-24 -5.44711f-28 -5.36697f-43; -1.52655f-24 -3.89561f-43 -7.56701f-43; 0.0 -5.60519f-44 -2.81661f-43]\n",
       "\n",
       "Float32[0.0 0.0 0.0; -4.48789f-25 -4.24301f-25 -2.05614f-26; 0.0 -6.83466f-28 -4.35285f-28]\n",
       "\n",
       "Float32[3.75717f-5 0.000130864 -0.000229126; 0.000326916 -1.90661f-5 -0.000156914; -6.24854f-8 -0.000324324 -1.36683f-5]\n",
       "\n",
       "Float32[0.000277271 0.000303523 8.8368f-5; 0.000621213 0.000467483 0.00025386; 0.000544283 -2.53242f-5 -0.000148691]\n",
       "\n",
       "Float32[0.00056989 -0.000519555 0.000235819; 0.000253019 -3.99703f-5 0.000165373; 0.000311186 0.000303504 2.66044f-5]\n",
       "\n",
       "...\n",
       "\n",
       "Float32[0.00019088 -0.000342844 0.000286487; 3.38153f-6 -0.000186297 7.9967f-7; 2.5167f-5 -6.05157f-7 8.58418f-7]\n",
       "\n",
       "Float32[-3.2967f-6 -0.000348342 -1.91949f-6; 6.06024f-6 6.05984f-6 4.12523f-6; 1.59356f-6 1.59356f-6 -1.22937f-7]\n",
       "\n",
       "Float32[-7.27273f-8 -7.27271f-8 -1.71383f-7; 0.00043295 0.000275736 0.000260691; 0.000194586 -0.000435041 -0.000277428]\n",
       "\n",
       "Float32[-1.00173f-12 -2.03716f-14 -4.31296f-18; -1.41892f-16 -2.8296f-20 -5.68099f-22; -2.12962f-20 -1.63335f-20 -1.56908f-23]\n",
       "\n",
       "Float32[-3.2075f-11 -3.04159f-11 -5.50647f-12; -2.70768f-11 -2.67906f-11 -5.51878f-12; -2.23013f-11 -2.08648f-11 -1.68943f-12]\n",
       "\n",
       "Float32[-2.2159f-11 -2.50396f-11 -1.92604f-11; -2.17499f-11 -2.26341f-11 -1.43282f-11; -8.07331f-12 -8.12051f-12 -7.84045f-12]\n",
       "\n",
       "...\n",
       "\n",
       "Float32[-2.68849f-13 -1.38222f-13 -2.45636f-21; -7.33479f-13 -2.67179f-13 -6.3747f-24; -1.49403f-12 -1.89536f-15 -2.13235f-34]\n",
       "\n",
       "Float32[-8.32825f-15 -2.21214f-13 -1.75932f-24; -1.139f-12 -2.59612f-13 -7.37834f-25; -1.53814f-12 -1.85755f-15 -8.143f-33]\n",
       "\n",
       "Float32[-1.19067f-11 -1.01521f-11 -8.78676f-13; -1.85454f-12 -6.81978f-13 -9.26242f-13; -3.84668f-12 -4.92519f-12 -4.98276f-12], Float32[-2.34896f-13]\n",
       "\n",
       "Float32[-6.79956f-25]\n",
       "\n",
       "Float32[1.01152f-11]\n",
       "\n",
       "Float32[-5.21817f-12]]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    \n",
       " nothing                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               "
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "∇loss(θs, ϕs, x, y_moves)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "search: \u001b[1mu\u001b[22m\u001b[1mp\u001b[22m\u001b[1md\u001b[22m\u001b[1ma\u001b[22m\u001b[1mt\u001b[22m\u001b[1me\u001b[22m\u001b[1m!\u001b[22m\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "```\n",
       "update!(weights, gradients, params)\n",
       "update!(weights, gradients; lr=0.001, gclip=0)\n",
       "```\n",
       "\n",
       "Update the `weights` using their `gradients` and the optimization algorithm parameters specified by `params`.  The 2-arg version defaults to the [`Sgd`](@ref) algorithm with learning rate `lr` and gradient clip `gclip`.  `gclip==0` indicates no clipping. The `weights` and possibly `gradients` and `params` are modified in-place.\n",
       "\n",
       "`weights` can be an individual numeric array or a collection of arrays represented by an iterator or dictionary.  In the individual case, `gradients` should be a similar numeric array of `size(weights)` and `params` should be a single object.  In the collection case, each individual weight array should have a corresponding params object. This way different weight arrays can have their own optimization state, different learning rates, or even different optimization algorithms running in parallel.  In the iterator case, `gradients` and `params` should be iterators of the same length as `weights` with corresponding elements.  In the dictionary case, `gradients` and `params` should be dictionaries with the same keys as `weights`.\n",
       "\n",
       "Individual optimization parameters can be one of the following types. The keyword arguments for each type's constructor and their default values are listed as well.\n",
       "\n",
       "  * [`Sgd`](@ref)`(;lr=0.001, gclip=0)`\n",
       "  * [`Momentum`](@ref)`(;lr=0.001, gclip=0, gamma=0.9)`\n",
       "  * [`Nesterov`](@ref)`(;lr=0.001, gclip=0, gamma=0.9)`\n",
       "  * [`Rmsprop`](@ref)`(;lr=0.001, gclip=0, rho=0.9, eps=1e-6)`\n",
       "  * [`Adagrad`](@ref)`(;lr=0.1, gclip=0, eps=1e-6)`\n",
       "  * [`Adadelta`](@ref)`(;lr=0.01, gclip=0, rho=0.9, eps=1e-6)`\n",
       "  * [`Adam`](@ref)`(;lr=0.001, gclip=0, beta1=0.9, beta2=0.999, eps=1e-8)`\n",
       "\n",
       "# Example:\n",
       "\n",
       "```\n",
       "w = rand(d)                 # an individual weight array\n",
       "g = lossgradient(w)         # gradient g has the same shape as w\n",
       "update!(w, g)               # update w in-place with Sgd()\n",
       "update!(w, g; lr=0.1)       # update w in-place with Sgd(lr=0.1)\n",
       "update!(w, g, Sgd(lr=0.1))  # update w in-place with Sgd(lr=0.1)\n",
       "\n",
       "w = (rand(d1), rand(d2))    # a tuple of weight arrays\n",
       "g = lossgradient2(w)        # g will also be a tuple\n",
       "p = (Adam(), Sgd())         # p has params for each w[i]\n",
       "update!(w, g, p)            # update each w[i] in-place with g[i],p[i]\n",
       "\n",
       "w = Any[rand(d1), rand(d2)] # any iterator can be used\n",
       "g = lossgradient3(w)        # g will be similar to w\n",
       "p = Any[Adam(), Sgd()]      # p should be an iterator of same length\n",
       "update!(w, g, p)            # update each w[i] in-place with g[i],p[i]\n",
       "\n",
       "w = Dict(:a => rand(d1), :b => rand(d2)) # dictionaries can be used\n",
       "g = lossgradient4(w)\n",
       "p = Dict(:a => Adam(), :b => Sgd())\n",
       "update!(w, g, p)\n",
       "```\n"
      ],
      "text/plain": [
       "```\n",
       "update!(weights, gradients, params)\n",
       "update!(weights, gradients; lr=0.001, gclip=0)\n",
       "```\n",
       "\n",
       "Update the `weights` using their `gradients` and the optimization algorithm parameters specified by `params`.  The 2-arg version defaults to the [`Sgd`](@ref) algorithm with learning rate `lr` and gradient clip `gclip`.  `gclip==0` indicates no clipping. The `weights` and possibly `gradients` and `params` are modified in-place.\n",
       "\n",
       "`weights` can be an individual numeric array or a collection of arrays represented by an iterator or dictionary.  In the individual case, `gradients` should be a similar numeric array of `size(weights)` and `params` should be a single object.  In the collection case, each individual weight array should have a corresponding params object. This way different weight arrays can have their own optimization state, different learning rates, or even different optimization algorithms running in parallel.  In the iterator case, `gradients` and `params` should be iterators of the same length as `weights` with corresponding elements.  In the dictionary case, `gradients` and `params` should be dictionaries with the same keys as `weights`.\n",
       "\n",
       "Individual optimization parameters can be one of the following types. The keyword arguments for each type's constructor and their default values are listed as well.\n",
       "\n",
       "  * [`Sgd`](@ref)`(;lr=0.001, gclip=0)`\n",
       "  * [`Momentum`](@ref)`(;lr=0.001, gclip=0, gamma=0.9)`\n",
       "  * [`Nesterov`](@ref)`(;lr=0.001, gclip=0, gamma=0.9)`\n",
       "  * [`Rmsprop`](@ref)`(;lr=0.001, gclip=0, rho=0.9, eps=1e-6)`\n",
       "  * [`Adagrad`](@ref)`(;lr=0.1, gclip=0, eps=1e-6)`\n",
       "  * [`Adadelta`](@ref)`(;lr=0.01, gclip=0, rho=0.9, eps=1e-6)`\n",
       "  * [`Adam`](@ref)`(;lr=0.001, gclip=0, beta1=0.9, beta2=0.999, eps=1e-8)`\n",
       "\n",
       "# Example:\n",
       "\n",
       "```\n",
       "w = rand(d)                 # an individual weight array\n",
       "g = lossgradient(w)         # gradient g has the same shape as w\n",
       "update!(w, g)               # update w in-place with Sgd()\n",
       "update!(w, g; lr=0.1)       # update w in-place with Sgd(lr=0.1)\n",
       "update!(w, g, Sgd(lr=0.1))  # update w in-place with Sgd(lr=0.1)\n",
       "\n",
       "w = (rand(d1), rand(d2))    # a tuple of weight arrays\n",
       "g = lossgradient2(w)        # g will also be a tuple\n",
       "p = (Adam(), Sgd())         # p has params for each w[i]\n",
       "update!(w, g, p)            # update each w[i] in-place with g[i],p[i]\n",
       "\n",
       "w = Any[rand(d1), rand(d2)] # any iterator can be used\n",
       "g = lossgradient3(w)        # g will be similar to w\n",
       "p = Any[Adam(), Sgd()]      # p should be an iterator of same length\n",
       "update!(w, g, p)            # update each w[i] in-place with g[i],p[i]\n",
       "\n",
       "w = Dict(:a => rand(d1), :b => rand(d2)) # dictionaries can be used\n",
       "g = lossgradient4(w)\n",
       "p = Dict(:a => Adam(), :b => Sgd())\n",
       "update!(w, g, p)\n",
       "```\n"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "?update!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1. Loss 0.005944858\n",
      "Epoch 2. Loss 0.0059297327\n",
      "Epoch 3. Loss 0.005914651\n",
      "Epoch 4. Loss 0.0058996016\n",
      "Epoch 5. Loss 0.005884637\n",
      "Epoch 6. Loss 0.0058696875\n",
      "Epoch 7. Loss 0.005854796\n",
      "Epoch 8. Loss 0.00583995\n",
      "Epoch 9. Loss 0.00582515\n",
      "Epoch 10. Loss 0.005810396\n",
      "Epoch 11. Loss 0.005795692\n",
      "Epoch 12. Loss 0.0057810247\n",
      "Epoch 13. Loss 0.00576642\n",
      "Epoch 14. Loss 0.005751835\n",
      "Epoch 15. Loss 0.0057373317\n",
      "Epoch 16. Loss 0.0057228506\n",
      "Epoch 17. Loss 0.0057084295\n",
      "Epoch 18. Loss 0.0056940494\n",
      "Epoch 19. Loss 0.0056797173\n",
      "Epoch 20. Loss 0.0056654247\n",
      "Epoch 21. Loss 0.0056511727\n",
      "Epoch 22. Loss 0.005636971\n",
      "Epoch 23. Loss 0.0056228214\n",
      "Epoch 24. Loss 0.0056086984\n",
      "Epoch 25. Loss 0.005594645\n",
      "Epoch 26. Loss 0.0055806153\n",
      "Epoch 27. Loss 0.005566643\n",
      "Epoch 28. Loss 0.005552715\n",
      "Epoch 29. Loss 0.005538832\n",
      "Epoch 30. Loss 0.0055249673\n",
      "Epoch 31. Loss 0.0055112005\n",
      "Epoch 32. Loss 0.0054974146\n",
      "Epoch 33. Loss 0.0054837065\n",
      "Epoch 34. Loss 0.0054700403\n",
      "Epoch 35. Loss 0.0054564066\n",
      "Epoch 36. Loss 0.0054428224\n",
      "Epoch 37. Loss 0.0054292944\n",
      "Epoch 38. Loss 0.0054157833\n",
      "Epoch 39. Loss 0.0054023433\n",
      "Epoch 40. Loss 0.0053889323\n",
      "Epoch 41. Loss 0.0053755664\n",
      "Epoch 42. Loss 0.0053622113\n",
      "Epoch 43. Loss 0.005348925\n",
      "Epoch 44. Loss 0.005335707\n",
      "Epoch 45. Loss 0.0053225\n",
      "Epoch 46. Loss 0.005309343\n",
      "Epoch 47. Loss 0.005296222\n",
      "Epoch 48. Loss 0.0052831424\n",
      "Epoch 49. Loss 0.0052701146\n",
      "Epoch 50. Loss 0.005257122\n",
      "Epoch 51. Loss 0.0052441703\n",
      "Epoch 52. Loss 0.005231246\n",
      "Epoch 53. Loss 0.005218378\n",
      "Epoch 54. Loss 0.005205556\n",
      "Epoch 55. Loss 0.005192746\n",
      "Epoch 56. Loss 0.0051800185\n",
      "Epoch 57. Loss 0.005167307\n",
      "Epoch 58. Loss 0.005154646\n",
      "Epoch 59. Loss 0.0051420177\n",
      "Epoch 60. Loss 0.005129433\n",
      "Epoch 61. Loss 0.0051168734\n",
      "Epoch 62. Loss 0.005104359\n",
      "Epoch 63. Loss 0.0050918926\n",
      "Epoch 64. Loss 0.0050794585\n",
      "Epoch 65. Loss 0.005067083\n",
      "Epoch 66. Loss 0.0050547286\n",
      "Epoch 67. Loss 0.0050424114\n",
      "Epoch 68. Loss 0.005030132\n",
      "Epoch 69. Loss 0.005017899\n",
      "Epoch 70. Loss 0.005005705\n",
      "Epoch 71. Loss 0.0049935435\n",
      "Epoch 72. Loss 0.0049814405\n",
      "Epoch 73. Loss 0.004969339\n",
      "Epoch 74. Loss 0.0049572964\n",
      "Epoch 75. Loss 0.0049453015\n",
      "Epoch 76. Loss 0.0049333517\n",
      "Epoch 77. Loss 0.004921424\n",
      "Epoch 78. Loss 0.0049095447\n",
      "Epoch 79. Loss 0.00489768\n",
      "Epoch 80. Loss 0.0048858738\n",
      "Epoch 81. Loss 0.0048740883\n",
      "Epoch 82. Loss 0.004862354\n",
      "Epoch 83. Loss 0.004850653\n",
      "Epoch 84. Loss 0.004838979\n",
      "Epoch 85. Loss 0.0048273713\n",
      "Epoch 86. Loss 0.0048157754\n",
      "Epoch 87. Loss 0.004804205\n",
      "Epoch 88. Loss 0.004792706\n",
      "Epoch 89. Loss 0.0047812224\n",
      "Epoch 90. Loss 0.004769779\n",
      "Epoch 91. Loss 0.0047583594\n",
      "Epoch 92. Loss 0.0047469884\n",
      "Epoch 93. Loss 0.0047356472\n",
      "Epoch 94. Loss 0.004724355\n",
      "Epoch 95. Loss 0.0047130915\n",
      "Epoch 96. Loss 0.004701847\n",
      "Epoch 97. Loss 0.004690671\n",
      "Epoch 98. Loss 0.0046794973\n",
      "Epoch 99. Loss 0.004668376\n",
      "Epoch 100. Loss 0.004657303\n"
     ]
    }
   ],
   "source": [
    "opt = Sgd(lr=0.001)\n",
    "for i in 1:100\n",
    "    li = loss(θs, ϕs, x, y_moves)\n",
    "    update!(θs, ∇loss(θs, ϕs, x, y_moves), [[opt, opt], [opt, opt]])\n",
    "    println(\"Epoch \", i, \". Loss \", li)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "loss (generic function with 2 methods)"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss(layer, fun, x) = sum(fun(layer[:params], x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "123.69244f0"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss(cl1, cl1[:transfer_function], x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(::gradfun) (generic function with 1 method)"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "∇loss = grad(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dict{Symbol,Any} with 1 entry:\n",
       "  :params => Dict(:bias=>Float32[63.0],:weights=>Float32[11.0 18.0 11.0; 16.0 2…"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "∇loss(cl1, cl1[:transfer_function], x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "ename": "LoadError",
     "evalue": "\u001b[91msyntax: invalid identifier name \".\"\u001b[39m",
     "output_type": "error",
     "traceback": [
      "\u001b[91msyntax: invalid identifier name \".\"\u001b[39m",
      "",
      "Stacktrace:",
      " [1] \u001b[1minclude_string\u001b[22m\u001b[22m\u001b[1m(\u001b[22m\u001b[22m::String, ::String\u001b[1m)\u001b[22m\u001b[22m at \u001b[1m./loading.jl:522\u001b[22m\u001b[22m"
     ]
    }
   ],
   "source": [
    "function bloss(params, ϕs, x)\n",
    "    for pf in zip(params, ϕs)\n",
    "        params, ϕ = pf\n",
    "        x = ϕ(params, x)\n",
    "    end\n",
    "    return sum(abs2(.x))\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12.12"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bloss([1], [(p, x) -> x], [12.12])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-5.2048396f11"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "update!([l[:params] for l in layers], ∇blossc(layers, x))\n",
    "bloss([l[:params] for l in layers], [l[:transfer_function] for l in layers], x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(::gradfun) (generic function with 1 method)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "∇bloss = grad(bloss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2-element Array{Any,1}:\n",
       " Any[Float32[-28.6034 -40.2424 -37.4153; -20.0241 -28.6625 -30.064; -13.6265 -19.3087 -17.5113]\n",
       "\n",
       "Float32[-27.2881 -36.007 -29.9811; -49.1222 -57.5985 -42.5337; -49.88 -64.661 -40.6141]\n",
       "\n",
       "Float32[0.0 0.0 0.0; 0.0 0.0 0.0; 0.0 0.0 0.0]\n",
       "\n",
       "Float32[0.0 0.0 0.0; 0.0 0.0 0.0; 0.0 0.0 0.0]\n",
       "\n",
       "Float32[-37.4564 -46.5109 -31.2603; -31.4305 -46.4542 -34.2165; -22.2058 -28.1182 -28.0615]\n",
       "\n",
       "Float32[-90.3349 -116.791 -95.7004; -97.5639 -126.746 -103.858; -82.6993 -106.119 -83.2307]\n",
       "\n",
       "Float32[0.0 0.0 0.0; 0.0 0.0 0.0; 0.0 0.0 0.0]\n",
       "\n",
       "Float32[-67.9433 -82.5746 -61.4839; -67.9433 -82.5746 -61.4839; -53.0788 -61.9474 -40.8567], Float32[-132.715]]\n",
       " Any[Float32[88.1307 107.896 83.9332; 99.9036 119.669 92.8819; 94.7341 113.877 88.0112]\n",
       "\n",
       "Float32[88.1307 107.896 83.9332; 99.9036 119.669 92.8819; 94.7341 113.877 88.0112], Float32[96.0]\n",
       "\n",
       "Float32[96.0]]                                                                                                                                                                                                                                                                                                                                                                                                                                         "
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "∇bloss([l[:params] for l in layers], [l[:transfer_function] for l in layers], x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "∇blossc (generic function with 1 method)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "∇blossc(layers, x) = ∇bloss([l[:params] for l in layers], [l[:transfer_function] for l in layers], x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2-element Array{Any,1}:\n",
       " Any[Float32[-28.6034 -40.2424 -37.4153; -20.0241 -28.6625 -30.064; -13.6265 -19.3087 -17.5113]\n",
       "\n",
       "Float32[-27.2881 -36.007 -29.9811; -49.1222 -57.5985 -42.5337; -49.88 -64.661 -40.6141]\n",
       "\n",
       "Float32[0.0 0.0 0.0; 0.0 0.0 0.0; 0.0 0.0 0.0]\n",
       "\n",
       "Float32[0.0 0.0 0.0; 0.0 0.0 0.0; 0.0 0.0 0.0]\n",
       "\n",
       "Float32[-37.4564 -46.5109 -31.2603; -31.4305 -46.4542 -34.2165; -22.2058 -28.1182 -28.0615]\n",
       "\n",
       "Float32[-90.3349 -116.791 -95.7004; -97.5639 -126.746 -103.858; -82.6993 -106.119 -83.2307]\n",
       "\n",
       "Float32[0.0 0.0 0.0; 0.0 0.0 0.0; 0.0 0.0 0.0]\n",
       "\n",
       "Float32[-67.9433 -82.5746 -61.4839; -67.9433 -82.5746 -61.4839; -53.0788 -61.9474 -40.8567], Float32[-132.715]]\n",
       " Any[Float32[88.1307 107.896 83.9332; 99.9036 119.669 92.8819; 94.7341 113.877 88.0112]\n",
       "\n",
       "Float32[88.1307 107.896 83.9332; 99.9036 119.669 92.8819; 94.7341 113.877 88.0112], Float32[96.0]\n",
       "\n",
       "Float32[96.0]]                                                                                                                                                                                                                                                                                                                                                                                                                                         "
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g = ∇blossc(layers, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "update!([l[:params] for l in layers], g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "ename": "LoadError",
     "evalue": "\u001b[91mMethodError: no method matching +(::Dict{Symbol,Array{Float32,4}}, ::Dict{Symbol,Any})\u001b[0m\nClosest candidates are:\n  +(::Any, ::Any, \u001b[91m::Any\u001b[39m, \u001b[91m::Any...\u001b[39m) at operators.jl:424\n  +(\u001b[91m::Type{AutoGrad.Grad{1}}\u001b[39m, ::Any, \u001b[91m::Any\u001b[39m, \u001b[91m::AutoGrad.Rec{##817}\u001b[39m) where ##817 at :0\n  +(\u001b[91m::Type{AutoGrad.Grad{1}}\u001b[39m, ::Any, \u001b[91m::Any\u001b[39m, \u001b[91m::AutoGrad.Rec{##835}\u001b[39m, \u001b[91m::AutoGrad.Rec{##836}\u001b[39m) where {##835, ##836} at :0\n  ...\u001b[39m",
     "output_type": "error",
     "traceback": [
      "\u001b[91mMethodError: no method matching +(::Dict{Symbol,Array{Float32,4}}, ::Dict{Symbol,Any})\u001b[0m\nClosest candidates are:\n  +(::Any, ::Any, \u001b[91m::Any\u001b[39m, \u001b[91m::Any...\u001b[39m) at operators.jl:424\n  +(\u001b[91m::Type{AutoGrad.Grad{1}}\u001b[39m, ::Any, \u001b[91m::Any\u001b[39m, \u001b[91m::AutoGrad.Rec{##817}\u001b[39m) where ##817 at :0\n  +(\u001b[91m::Type{AutoGrad.Grad{1}}\u001b[39m, ::Any, \u001b[91m::Any\u001b[39m, \u001b[91m::AutoGrad.Rec{##835}\u001b[39m, \u001b[91m::AutoGrad.Rec{##836}\u001b[39m) where {##835, ##836} at :0\n  ...\u001b[39m",
      "",
      "Stacktrace:",
      " [1] \u001b[1mbroadcast_t\u001b[22m\u001b[22m\u001b[1m(\u001b[22m\u001b[22m::Function, ::Type{Any}, ::Tuple{Base.OneTo{Int64}}, ::CartesianRange{CartesianIndex{1}}, ::Array{Dict{Symbol,Array{Float32,4}},1}, ::Array{Any,1}\u001b[1m)\u001b[22m\u001b[22m at \u001b[1m./broadcast.jl:258\u001b[22m\u001b[22m",
      " [2] \u001b[1mbroadcast_c\u001b[22m\u001b[22m at \u001b[1m./broadcast.jl:321\u001b[22m\u001b[22m [inlined]",
      " [3] \u001b[1mbroadcast\u001b[22m\u001b[22m\u001b[1m(\u001b[22m\u001b[22m::Function, ::Array{Dict{Symbol,Array{Float32,4}},1}, ::Array{Any,1}\u001b[1m)\u001b[22m\u001b[22m at \u001b[1m/home/bbethke/.julia/v0.6/AutoGrad/src/unfuse.jl:35\u001b[22m\u001b[22m",
      " [4] \u001b[1minclude_string\u001b[22m\u001b[22m\u001b[1m(\u001b[22m\u001b[22m::String, ::String\u001b[1m)\u001b[22m\u001b[22m at \u001b[1m./loading.jl:522\u001b[22m\u001b[22m"
     ]
    }
   ],
   "source": [
    "[l[:params] for l in layers] .+ g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functional model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "predict (generic function with 1 method)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function predict(w, x)\n",
    "    n = 100\n",
    "    for i in 1:n\n",
    "        x = cat(3, x, relu.( conv4(w[i][1], x, padding=1) .+ w[i][2] ))\n",
    "    end\n",
    "    x = relu.( conv4(w[n+1][1], x, padding=1) .+ w[n+1][2] )\n",
    "    x = reshape(x, 8*4*4, size(x, 4))\n",
    "    return softmax(x, 1)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "nc(i) = 8 + 4(i - 1)\n",
    "w = vcat(\n",
    "    [(xavier(Float32, 3, 3, nc(i), 4), xavier(Float32, 1, 1, 4, 1)) for i in 1:100],\n",
    "    [(xavier(Float32, 3, 3, nc(101), 4), xavier(Float32, 1, 1, 4, 1))]\n",
    ");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "softmax (generic function with 1 method)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function softmax(x, axes)\n",
    "    exp_x = exp.(x .- maximum(x, axes))\n",
    "    return exp_x ./ sum(exp_x, axes)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "kl_divergence (generic function with 1 method)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function kl_divergence(p, q)\n",
    "    ppe = p .+ 0.0001f0\n",
    "    return -sum(ppe .* log.(q ./ ppe), 1)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "loss (generic function with 1 method)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss(w, x, y) = mean(kl_divergence(reshape(y, 8*4*4, size(y, 4)), predict(w, x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(::gradfun) (generic function with 1 method)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "∇loss = grad(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@time ∇loss(w, x, y_moves);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@time loss(w, x, y_moves)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = optimizers(w, Sgd, lr=0.001);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch loss 3.5833597\n",
      "Batch loss 8.052206\n",
      "Batch loss 2.891875\n",
      "Batch loss 4.122321\n",
      "Batch loss 3.2124307\n",
      "Batch loss 3.4684405\n",
      "Batch loss 3.2266831\n",
      "Batch loss 3.465651\n",
      "Batch loss 3.8753717\n",
      "Batch loss 3.4895737\n",
      "Batch loss 2.9162836\n",
      "Batch loss 3.3147407\n",
      "Batch loss 2.9968133\n"
     ]
    },
    {
     "ename": "LoadError",
     "evalue": "\u001b[91mInterruptException:\u001b[39m",
     "output_type": "error",
     "traceback": [
      "\u001b[91mInterruptException:\u001b[39m",
      "",
      "Stacktrace:",
      " [1] \u001b[1mupdate!\u001b[22m\u001b[22m\u001b[1m(\u001b[22m\u001b[22m::Tuple{Array{Float32,4},Array{Float32,4}}, ::Tuple{Array{Float32,4},Array{Float32,4}}, ::Tuple{Knet.Sgd,Knet.Sgd}\u001b[1m)\u001b[22m\u001b[22m at \u001b[1m/home/bbethke/.julia/v0.6/Knet/src/update.jl:440\u001b[22m\u001b[22m",
      " [2] \u001b[1mupdate!\u001b[22m\u001b[22m\u001b[1m(\u001b[22m\u001b[22m::Array{Tuple{Array{Float32,4},Array{Float32,4}},1}, ::Array{Any,1}, ::Array{Tuple{Knet.Sgd,Knet.Sgd},1}\u001b[1m)\u001b[22m\u001b[22m at \u001b[1m/home/bbethke/.julia/v0.6/Knet/src/update.jl:441\u001b[22m\u001b[22m",
      " [3] \u001b[1mmacro expansion\u001b[22m\u001b[22m at \u001b[1m./In[12]:4\u001b[22m\u001b[22m [inlined]",
      " [4] \u001b[1manonymous\u001b[22m\u001b[22m at \u001b[1m./<missing>:?\u001b[22m\u001b[22m",
      " [5] \u001b[1minclude_string\u001b[22m\u001b[22m\u001b[1m(\u001b[22m\u001b[22m::String, ::String\u001b[1m)\u001b[22m\u001b[22m at \u001b[1m./loading.jl:522\u001b[22m\u001b[22m"
     ]
    }
   ],
   "source": [
    "for epoch in 1:100\n",
    "    for batch in minibatch(x, y_moves, 4, shuffle=true, partial=true)\n",
    "        mini_x, mini_y = batch\n",
    "        update!(w, ∇loss(w, mini_x, mini_y), opt)\n",
    "        println(\"Batch loss $(loss(w, mini_x, mini_y))\")\n",
    "    end\n",
    "    println(\"Epoch $epoch, loss $(loss(w, x, y_moves))\")\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "128×1598 Array{Float32,2}:\n",
       " 0.0  0.0  0.0      0.0       0.0       …  0.0  0.0       0.0       0.0  0.0\n",
       " 0.0  0.0  0.0      0.0       0.0          0.0  0.0       0.0       0.0  0.0\n",
       " 0.0  0.0  0.0      0.0       0.0          0.0  0.0       0.0       0.0  0.0\n",
       " 0.0  0.0  0.0      0.0       0.0          0.0  0.0       0.0       0.0  0.0\n",
       " 0.0  0.0  0.14563  0.0       0.108337     0.0  0.265381  0.0       0.0  0.0\n",
       " 0.0  0.0  0.0      0.0       0.0       …  0.0  0.0       0.0       0.0  0.0\n",
       " 0.0  0.0  0.0      0.0       0.0          0.0  0.0       0.0       0.0  0.0\n",
       " 0.0  0.0  0.0      0.0       0.0          0.0  0.0       0.0       0.0  0.0\n",
       " 0.0  0.0  0.0      0.0       0.0          0.0  0.0       0.0       0.0  0.0\n",
       " 0.0  0.0  0.0      0.0       0.0          0.0  0.0       0.0       0.0  0.0\n",
       " 0.0  0.0  0.0      0.0       0.0       …  0.0  0.0       0.0       0.0  0.0\n",
       " 0.0  0.0  0.0      0.0       0.0          0.0  0.0       0.0       0.0  0.0\n",
       " 0.0  0.0  0.0      0.0       0.0          0.0  0.0       0.111328  0.0  0.0\n",
       " ⋮                                      ⋱                 ⋮                 \n",
       " 0.0  0.0  0.0      0.0       0.0          0.0  0.0       0.0       0.0  0.0\n",
       " 0.0  0.0  0.0      0.0       0.0          0.0  0.0       0.0       0.0  0.0\n",
       " 0.0  0.0  0.0      0.0       0.0          0.0  0.0       0.0       0.0  0.0\n",
       " 0.0  0.0  0.0      0.0       0.0          0.0  0.0       0.0       0.0  0.0\n",
       " 0.0  0.0  0.0      0.0       0.0       …  0.0  0.0       0.0       0.0  0.0\n",
       " 0.0  0.0  0.0      0.370117  0.0          0.0  0.0       0.0       0.0  0.0\n",
       " 0.0  0.0  0.0      0.0       0.0          0.0  0.0       0.0       0.0  0.0\n",
       " 0.0  0.0  0.0      0.38208   0.0          0.0  0.0       0.0       0.0  0.0\n",
       " 0.0  0.0  0.0      0.0       0.0          0.0  0.0       0.0       0.0  0.0\n",
       " 0.0  0.0  0.0      0.0       0.0       …  0.0  0.0       0.0       0.0  0.0\n",
       " 0.0  0.0  0.0      0.0       0.0          0.0  0.0       0.0       0.0  0.0\n",
       " 0.0  0.0  0.0      0.0       0.0          0.0  0.0       0.0       0.0  0.0"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reshape(y_moves, 8*4*4, :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "LoadError",
     "evalue": "\u001b[91mUndefVarError: softmax not defined\u001b[39m",
     "output_type": "error",
     "traceback": [
      "\u001b[91mUndefVarError: softmax not defined\u001b[39m",
      "",
      "Stacktrace:",
      " [1] \u001b[1mpredict\u001b[22m\u001b[22m\u001b[1m(\u001b[22m\u001b[22m::Array{Tuple{Array{Float32,4},Array{Float32,4}},1}, ::Array{Float32,4}\u001b[1m)\u001b[22m\u001b[22m at \u001b[1m./In[5]:8\u001b[22m\u001b[22m",
      " [2] \u001b[1minclude_string\u001b[22m\u001b[22m\u001b[1m(\u001b[22m\u001b[22m::String, ::String\u001b[1m)\u001b[22m\u001b[22m at \u001b[1m./loading.jl:522\u001b[22m\u001b[22m"
     ]
    }
   ],
   "source": [
    "predict(w, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "LoadError",
     "evalue": "\u001b[91mUndefVarError: softmax not defined\u001b[39m",
     "output_type": "error",
     "traceback": [
      "\u001b[91mUndefVarError: softmax not defined\u001b[39m",
      "",
      "Stacktrace:",
      " [1] \u001b[1mpredict\u001b[22m\u001b[22m\u001b[1m(\u001b[22m\u001b[22m::Array{Tuple{Array{Float32,4},Array{Float32,4}},1}, ::Array{Float32,4}\u001b[1m)\u001b[22m\u001b[22m at \u001b[1m./In[5]:8\u001b[22m\u001b[22m",
      " [2] \u001b[1mloss\u001b[22m\u001b[22m\u001b[1m(\u001b[22m\u001b[22m::Array{Tuple{Array{Float32,4},Array{Float32,4}},1}, ::Array{Float32,4}, ::Array{Float32,4}\u001b[1m)\u001b[22m\u001b[22m at \u001b[1m./In[8]:1\u001b[22m\u001b[22m",
      " [3] \u001b[1minclude_string\u001b[22m\u001b[22m\u001b[1m(\u001b[22m\u001b[22m::String, ::String\u001b[1m)\u001b[22m\u001b[22m at \u001b[1m./loading.jl:522\u001b[22m\u001b[22m"
     ]
    }
   ],
   "source": [
    "loss(w, x, y_moves)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 0.6.2",
   "language": "julia",
   "name": "julia-0.6"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "0.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
